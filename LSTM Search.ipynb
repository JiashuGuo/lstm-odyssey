{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# seq2seq provides our loss fn\n",
    "from tensorflow.models.rnn import seq2seq\n",
    "\n",
    "# parses the dataset\n",
    "import ptb_reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, _ = ptb_reader.ptb_raw_data(\"ptb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self, CellType, is_training, config):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "        cell = CellType(size)\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # initializer used for reusable variable initializer (see `get_variable`)\n",
    "        initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, size], initializer=initializer)\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "\n",
    "        outputs = []\n",
    "        states = []\n",
    "        state = self.initial_state\n",
    "\n",
    "        with tf.variable_scope(\"RNN\", initializer=initializer):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                inputs_slice = inputs[:,time_step,:]\n",
    "                (cell_output, state) = cell(inputs_slice, state)\n",
    "\n",
    "                outputs.append(cell_output)\n",
    "                states.append(state)\n",
    "\n",
    "        self.final_state = states[-1]\n",
    "\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1, size])\n",
    "        w = tf.get_variable(\"softmax_w\",\n",
    "                                    [size, vocab_size],\n",
    "                                    initializer=initializer)\n",
    "        b = tf.get_variable(\"softmax_b\", [vocab_size], initializer=initializer)\n",
    "\n",
    "        logits = tf.nn.xw_plus_b(output, w, b) # compute logits for loss\n",
    "        targets = tf.reshape(self.targets, [-1]) # reshape our target outputs\n",
    "        weights = tf.ones([batch_size * num_steps]) # used to scale the loss average\n",
    "\n",
    "        # computes loss and performs softmax on our fully-connected output layer\n",
    "        loss = sequence_loss_by_example([logits], [targets], [weights], vocab_size)\n",
    "        self.cost = cost = tf.reduce_sum(loss) / batch_size\n",
    "\n",
    "        if is_training:\n",
    "            # define training operation and clip the gradients\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "            optimizer = tf.train.MomentumOptimizer(config.learning_rate, config.momentum)\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        else:\n",
    "            # if this model isn't for training (i.e. testing/validation) then we don't do anything here\n",
    "            self.train_op = tf.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(sess, model, data, is_training=False, verbose=False):\n",
    "    epoch_size = ((len(data) // model.batch_size) - 1) // model.num_steps\n",
    "    start_time = time.time()\n",
    "\n",
    "    # accumulated counts\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    # initial RNN state\n",
    "    state = model.initial_state.eval()\n",
    "\n",
    "    for step, (x, y) in enumerate(ptb_reader.ptb_iterator(data, model.batch_size, model.num_steps)):\n",
    "        cost, state, _ = sess.run([model.cost, model.final_state, model.train_op], {\n",
    "            model.input_data: x,\n",
    "            model.targets: y,\n",
    "            model.initial_state: state\n",
    "        })\n",
    "        costs += cost\n",
    "        iters += model.num_steps\n",
    "\n",
    "        perplexity = np.exp(costs / iters)\n",
    "\n",
    "        if verbose and step % 10 == 0:\n",
    "            progress = (step / epoch_size) * 100\n",
    "            wps = iters * model.batch_size / (time.time() - start_time)\n",
    "            print(\"%.1f%% perplexity: %.3f speed: %.0f wps\" % (progress, perplexity, wps))\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    batch_size = 30\n",
    "    num_steps = 20\n",
    "    hidden_size = 200\n",
    "    vocab_size = 10000\n",
    "    learning_rate = 1e-1\n",
    "    momentum = 0.9\n",
    "    max_grad_norm = 5\n",
    "    init_scale = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default settings for training\n",
    "train_config = Config()\n",
    "\n",
    "# our evaluation runs (validation and testing), use a batch size and time step of one\n",
    "eval_config = Config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1\n",
    "\n",
    "# number of training epochs to perform over the training data\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here we import and specify our cell variant\n",
    "# (all variants are subclasses of tensorflow.models.rnn.rnn_cell.RNNCell)\n",
    "from variants.vanilla import VanillaLSTMCell\n",
    "CellType = VanillaLSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% perplexity: 10044.200 speed: 593 wps\n",
      "0.6% perplexity: 5123.211 speed: 825 wps\n",
      "1.3% perplexity: 2692.925 speed: 782 wps\n",
      "1.9% perplexity: 1954.400 speed: 797 wps\n",
      "2.6% perplexity: 1638.685 speed: 817 wps\n",
      "3.2% perplexity: 1410.521 speed: 830 wps\n",
      "3.9% perplexity: 1270.584 speed: 821 wps\n",
      "4.5% perplexity: 1150.892 speed: 826 wps\n",
      "5.2% perplexity: 1063.641 speed: 835 wps\n",
      "5.8% perplexity: 1011.578 speed: 842 wps\n",
      "6.5% perplexity: 953.236 speed: 846 wps\n",
      "7.1% perplexity: 909.500 speed: 843 wps\n",
      "7.7% perplexity: 875.879 speed: 843 wps\n",
      "8.4% perplexity: 841.867 speed: 834 wps\n",
      "9.0% perplexity: 814.163 speed: 835 wps\n",
      "9.7% perplexity: 789.053 speed: 833 wps\n",
      "10.3% perplexity: 763.716 speed: 832 wps\n",
      "11.0% perplexity: 746.881 speed: 830 wps\n",
      "11.6% perplexity: 728.605 speed: 827 wps\n",
      "12.3% perplexity: 710.804 speed: 827 wps\n",
      "12.9% perplexity: 692.337 speed: 832 wps\n",
      "13.6% perplexity: 675.117 speed: 836 wps\n",
      "14.2% perplexity: 657.138 speed: 841 wps\n",
      "14.8% perplexity: 643.408 speed: 845 wps\n",
      "15.5% perplexity: 628.381 speed: 848 wps\n",
      "16.1% perplexity: 617.229 speed: 852 wps\n",
      "16.8% perplexity: 605.406 speed: 856 wps\n",
      "17.4% perplexity: 594.768 speed: 858 wps\n",
      "18.1% perplexity: 584.089 speed: 861 wps\n",
      "18.7% perplexity: 576.715 speed: 864 wps\n",
      "19.4% perplexity: 567.711 speed: 867 wps\n",
      "20.0% perplexity: 559.520 speed: 867 wps\n",
      "20.7% perplexity: 548.688 speed: 867 wps\n",
      "21.3% perplexity: 540.101 speed: 868 wps\n",
      "21.9% perplexity: 531.779 speed: 869 wps\n",
      "22.6% perplexity: 522.977 speed: 869 wps\n",
      "23.2% perplexity: 516.223 speed: 869 wps\n",
      "23.9% perplexity: 511.487 speed: 869 wps\n",
      "24.5% perplexity: 506.337 speed: 869 wps\n",
      "25.2% perplexity: 499.572 speed: 868 wps\n",
      "25.8% perplexity: 492.129 speed: 868 wps\n",
      "26.5% perplexity: 486.260 speed: 868 wps\n",
      "27.1% perplexity: 480.515 speed: 868 wps\n",
      "27.8% perplexity: 475.056 speed: 868 wps\n",
      "28.4% perplexity: 469.383 speed: 867 wps\n",
      "29.1% perplexity: 465.310 speed: 867 wps\n",
      "29.7% perplexity: 460.936 speed: 868 wps\n",
      "30.3% perplexity: 456.234 speed: 868 wps\n",
      "31.0% perplexity: 452.939 speed: 868 wps\n",
      "31.6% perplexity: 448.203 speed: 865 wps\n",
      "32.3% perplexity: 443.353 speed: 865 wps\n",
      "32.9% perplexity: 439.469 speed: 865 wps\n",
      "33.6% perplexity: 434.425 speed: 863 wps\n",
      "34.2% perplexity: 430.470 speed: 864 wps\n",
      "34.9% perplexity: 426.212 speed: 863 wps\n",
      "35.5% perplexity: 421.535 speed: 864 wps\n",
      "36.2% perplexity: 415.764 speed: 864 wps\n",
      "36.8% perplexity: 411.200 speed: 864 wps\n",
      "37.4% perplexity: 408.088 speed: 864 wps\n",
      "38.1% perplexity: 404.797 speed: 864 wps\n",
      "38.7% perplexity: 401.742 speed: 864 wps\n",
      "39.4% perplexity: 399.360 speed: 864 wps\n",
      "40.0% perplexity: 396.982 speed: 865 wps\n",
      "40.7% perplexity: 393.703 speed: 865 wps\n",
      "41.3% perplexity: 390.701 speed: 865 wps\n",
      "42.0% perplexity: 387.622 speed: 865 wps\n",
      "42.6% perplexity: 385.064 speed: 865 wps\n",
      "43.3% perplexity: 381.745 speed: 865 wps\n",
      "43.9% perplexity: 379.337 speed: 865 wps\n",
      "44.5% perplexity: 378.215 speed: 865 wps\n",
      "45.2% perplexity: 376.591 speed: 863 wps\n",
      "45.8% perplexity: 375.050 speed: 862 wps\n",
      "46.5% perplexity: 372.977 speed: 863 wps\n",
      "47.1% perplexity: 371.032 speed: 861 wps\n",
      "47.8% perplexity: 368.551 speed: 860 wps\n",
      "48.4% perplexity: 366.013 speed: 860 wps\n",
      "49.1% perplexity: 363.585 speed: 860 wps\n",
      "49.7% perplexity: 361.564 speed: 860 wps\n",
      "50.4% perplexity: 359.904 speed: 860 wps\n",
      "51.0% perplexity: 358.873 speed: 858 wps\n",
      "51.6% perplexity: 357.416 speed: 858 wps\n",
      "52.3% perplexity: 355.510 speed: 858 wps\n",
      "52.9% perplexity: 353.895 speed: 858 wps\n",
      "53.6% perplexity: 351.854 speed: 858 wps\n",
      "54.2% perplexity: 349.728 speed: 858 wps\n",
      "54.9% perplexity: 347.771 speed: 858 wps\n",
      "55.5% perplexity: 346.284 speed: 858 wps\n",
      "56.2% perplexity: 344.843 speed: 859 wps\n",
      "56.8% perplexity: 343.486 speed: 859 wps\n",
      "57.5% perplexity: 341.606 speed: 858 wps\n",
      "58.1% perplexity: 339.731 speed: 858 wps\n",
      "58.7% perplexity: 337.355 speed: 858 wps\n",
      "59.4% perplexity: 334.900 speed: 858 wps\n",
      "60.0% perplexity: 333.128 speed: 857 wps\n",
      "60.7% perplexity: 331.498 speed: 857 wps\n",
      "61.3% perplexity: 329.521 speed: 857 wps\n",
      "62.0% perplexity: 327.567 speed: 856 wps\n",
      "62.6% perplexity: 325.736 speed: 856 wps\n",
      "63.3% perplexity: 324.211 speed: 857 wps\n",
      "63.9% perplexity: 322.795 speed: 857 wps\n",
      "64.6% perplexity: 321.230 speed: 857 wps\n",
      "65.2% perplexity: 319.852 speed: 857 wps\n",
      "65.8% perplexity: 319.256 speed: 857 wps\n",
      "66.5% perplexity: 318.292 speed: 858 wps\n",
      "67.1% perplexity: 317.299 speed: 857 wps\n",
      "67.8% perplexity: 316.442 speed: 856 wps\n",
      "68.4% perplexity: 315.170 speed: 855 wps\n",
      "69.1% perplexity: 314.183 speed: 854 wps\n",
      "69.7% perplexity: 313.301 speed: 855 wps\n",
      "70.4% perplexity: 312.158 speed: 855 wps\n",
      "71.0% perplexity: 311.035 speed: 855 wps\n",
      "71.7% perplexity: 310.117 speed: 855 wps\n",
      "72.3% perplexity: 308.877 speed: 856 wps\n",
      "73.0% perplexity: 307.686 speed: 855 wps\n",
      "73.6% perplexity: 306.613 speed: 856 wps\n",
      "74.2% perplexity: 305.534 speed: 856 wps\n",
      "74.9% perplexity: 304.326 speed: 857 wps\n",
      "75.5% perplexity: 303.381 speed: 858 wps\n",
      "76.2% perplexity: 302.263 speed: 858 wps\n",
      "76.8% perplexity: 301.512 speed: 859 wps\n",
      "77.5% perplexity: 300.689 speed: 860 wps\n",
      "78.1% perplexity: 299.776 speed: 860 wps\n",
      "78.8% perplexity: 298.780 speed: 861 wps\n",
      "79.4% perplexity: 298.040 speed: 861 wps\n",
      "80.1% perplexity: 297.045 speed: 862 wps\n",
      "80.7% perplexity: 296.385 speed: 863 wps\n",
      "81.3% perplexity: 295.449 speed: 863 wps\n",
      "82.0% perplexity: 294.114 speed: 864 wps\n",
      "82.6% perplexity: 293.121 speed: 864 wps\n",
      "83.3% perplexity: 292.277 speed: 865 wps\n",
      "83.9% perplexity: 291.006 speed: 865 wps\n",
      "84.6% perplexity: 289.767 speed: 865 wps\n",
      "85.2% perplexity: 288.407 speed: 866 wps\n",
      "85.9% perplexity: 287.116 speed: 866 wps\n",
      "86.5% perplexity: 285.789 speed: 866 wps\n",
      "87.2% perplexity: 284.541 speed: 866 wps\n",
      "87.8% perplexity: 283.723 speed: 866 wps\n",
      "88.4% perplexity: 282.821 speed: 866 wps\n",
      "89.1% perplexity: 282.280 speed: 866 wps\n",
      "89.7% perplexity: 281.844 speed: 866 wps\n",
      "90.4% perplexity: 281.045 speed: 866 wps\n",
      "91.0% perplexity: 280.250 speed: 866 wps\n",
      "91.7% perplexity: 278.982 speed: 865 wps\n",
      "92.3% perplexity: 278.087 speed: 865 wps\n",
      "93.0% perplexity: 277.241 speed: 865 wps\n",
      "93.6% perplexity: 276.084 speed: 865 wps\n",
      "94.3% perplexity: 275.166 speed: 864 wps\n",
      "94.9% perplexity: 274.215 speed: 864 wps\n",
      "95.5% perplexity: 273.332 speed: 865 wps\n",
      "96.2% perplexity: 272.532 speed: 865 wps\n",
      "96.8% perplexity: 271.988 speed: 865 wps\n",
      "97.5% perplexity: 271.398 speed: 865 wps\n",
      "98.1% perplexity: 270.772 speed: 866 wps\n",
      "98.8% perplexity: 270.108 speed: 865 wps\n",
      "99.4% perplexity: 269.557 speed: 866 wps\n",
      "0 training complete, perplexity: 269.083\n",
      "0 validation complete, perplexity: 185.083\n",
      "0.0% perplexity: 257.518 speed: 855 wps\n",
      "0.6% perplexity: 205.154 speed: 937 wps\n",
      "1.3% perplexity: 189.645 speed: 890 wps\n",
      "1.9% perplexity: 179.022 speed: 875 wps"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    # define our training model\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        train_model = PTBModel(CellType, is_training=True, config=train_config)\n",
    "\n",
    "    # we create a separate model for validation and testing to alter the batch size and time steps\n",
    "    # reuse=True reuses variables from the previously defined `train_model`\n",
    "    with tf.variable_scope(\"model\", reuse=True):\n",
    "        valid_model = PTBModel(CellType, is_training=False, config=train_config)\n",
    "        test_model = PTBModel(CellType, is_training=False, config=eval_config)\n",
    "\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        # run training pass\n",
    "        train_perplexity = run_epoch(sess, train_model, train_data, verbose=True)\n",
    "        print(\"%i training complete, perplexity: %.3f\" % (i, train_perplexity))\n",
    "\n",
    "        # run validation pass\n",
    "        valid_perplexity = run_epoch(sess, valid_model, valid_data)\n",
    "        print(\"%i validation complete, perplexity: %.3f\" % (i, valid_perplexity))\n",
    "\n",
    "    # run test pass\n",
    "    test_perplexity = run_epoch(sess, test_model, test_data)\n",
    "    print(\"testing complete, perplexity: %.3f\" % (test_perplexity,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
