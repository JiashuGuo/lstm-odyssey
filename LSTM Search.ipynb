{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn.seq2seq import sequence_loss_by_example\n",
    "\n",
    "# parses the dataset\n",
    "import ptb_reader\n",
    "\n",
    "# import variants\n",
    "from variants.vanilla import VanillaLSTMCell\n",
    "from variants.nig import NIGLSTMCell\n",
    "from variants.nfg import NFGLSTMCell\n",
    "from variants.nog import NOGLSTMCell\n",
    "from variants.niaf import NIAFLSTMCell\n",
    "from variants.noaf import NOAFLSTMCell\n",
    "from variants.np import NPLSTMCell\n",
    "from variants.cifg import CIFGLSTMCell\n",
    "from variants.fgr import FGRLSTMCell\n",
    "\n",
    "cell_types = {\n",
    "    'vanilla': VanillaLSTMCell,\n",
    "    'nig': NIGLSTMCell,\n",
    "    'nfg': NFGLSTMCell,\n",
    "    'nog': NOGLSTMCell,\n",
    "    'niaf': NIAFLSTMCell,\n",
    "    'noaf': NOAFLSTMCell,\n",
    "    'np': NPLSTMCell,\n",
    "    'cifg': CIFGLSTMCell,\n",
    "    'fgr': FGRLSTMCell,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, _ = ptb_reader.ptb_raw_data(\"ptb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self, CellType, is_training, config):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"input_data\")\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"targets\")\n",
    "\n",
    "        lstm_cell = CellType(size)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            lstm_cell = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob)\n",
    "        cell = rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers)\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # initializer used for reusable variable initializer (see `get_variable`)\n",
    "        initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, size], initializer=initializer)\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "        outputs = []\n",
    "        states = []\n",
    "        state = self.initial_state\n",
    "\n",
    "        with tf.variable_scope(\"RNN\", initializer=initializer):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                inputs_slice = inputs[:,time_step,:]\n",
    "                (cell_output, state) = cell(inputs_slice, state)\n",
    "\n",
    "                outputs.append(cell_output)\n",
    "                states.append(state)\n",
    "\n",
    "        self.final_state = states[-1]\n",
    "\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1, size])\n",
    "        w = tf.get_variable(\"softmax_w\",\n",
    "                                    [size, vocab_size],\n",
    "                                    initializer=initializer)\n",
    "        b = tf.get_variable(\"softmax_b\", [vocab_size], initializer=initializer)\n",
    "\n",
    "        logits = tf.nn.xw_plus_b(output, w, b) # compute logits for loss\n",
    "        targets = tf.reshape(self.targets, [-1]) # reshape our target outputs\n",
    "        weights = tf.ones([batch_size * num_steps]) # used to scale the loss average\n",
    "\n",
    "        # computes loss and performs softmax on our fully-connected output layer\n",
    "        loss = sequence_loss_by_example([logits], [targets], [weights], vocab_size)\n",
    "        self.cost = cost = tf.div(tf.reduce_sum(loss), batch_size, name=\"cost\")\n",
    "\n",
    "        if is_training:\n",
    "            # setup learning rate variable to decay\n",
    "            self.lr = tf.Variable(1.0, trainable=False)\n",
    "\n",
    "            # define training operation and clip the gradients\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars), name=\"train\")\n",
    "        else:\n",
    "            # if this model isn't for training (i.e. testing/validation) then we don't do anything here\n",
    "            self.train_op = tf.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(sess, model, data, verbose=False):\n",
    "    epoch_size = ((len(data) // model.batch_size) - 1) // model.num_steps\n",
    "    start_time = time.time()\n",
    "\n",
    "    # accumulated counts\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    # initial RNN state\n",
    "    state = model.initial_state.eval()\n",
    "\n",
    "    for step, (x, y) in enumerate(ptb_reader.ptb_iterator(data, model.batch_size, model.num_steps)):\n",
    "        cost, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed_dict={\n",
    "            model.input_data: x,\n",
    "            model.targets: y,\n",
    "            model.initial_state: state\n",
    "        })\n",
    "        costs += cost\n",
    "        iters += model.num_steps\n",
    "\n",
    "        perplexity = np.exp(costs / iters)\n",
    "\n",
    "        if verbose and step % 10 == 0:\n",
    "            progress = (step / epoch_size) * 100\n",
    "            wps = iters * model.batch_size / (time.time() - start_time)\n",
    "            print(\"%.1f%% Perplexity: %.3f (Cost: %.3f) Speed: %.0f wps\" % (progress, perplexity, cost, wps))\n",
    "\n",
    "    return (costs / iters), perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    batch_size = 20\n",
    "    num_steps = 35 # number of unrolled time steps\n",
    "    hidden_size = 450 # number of blocks in an LSTM cell\n",
    "    vocab_size = 10000\n",
    "    max_grad_norm = 5 # maximum gradient for clipping\n",
    "    init_scale = 0.05 # scale between -0.1 and 0.1 for all random initialization\n",
    "    keep_prob = 0.5 # dropout probability\n",
    "    num_layers = 2 # number of LSTM layers\n",
    "    learning_rate = 1.0\n",
    "    lr_decay = 0.8\n",
    "    lr_decay_epoch_offset = 6 # don't decay until after the Nth epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default settings for training\n",
    "train_config = Config()\n",
    "\n",
    "# our evaluation runs (validation and testing), use a batch size and time step of one\n",
    "eval_config = Config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1\n",
    "\n",
    "# number of epochs to perform over the training data\n",
    "num_epochs = 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell Variant\n",
    "\n",
    "Here we import and specify our cell variant. Note that aall variants are subclasses of `tensorflow.models.rnn.rnn_cell.RNNCell`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = \"vanilla\"\n",
    "CellType = cell_types[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    # define our training model\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        train_model = PTBModel(CellType, is_training=True, config=train_config)\n",
    "\n",
    "    # we create a separate model for validation and testing to alter the batch size and time steps\n",
    "    # reuse=True reuses variables from the previously defined `train_model`\n",
    "    with tf.variable_scope(\"model\", reuse=True):\n",
    "        valid_model = PTBModel(CellType, is_training=False, config=train_config)\n",
    "        test_model = PTBModel(CellType, is_training=False, config=eval_config)\n",
    "\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    tf.train.write_graph(sess.graph_def, 'models/', '%s.pb' % model_name, as_text=False)\n",
    "\n",
    "    train_costs = []\n",
    "    train_perps = []\n",
    "    valid_costs = []\n",
    "    valid_perps = []\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        # decay learning rate each epoch\n",
    "        print(\"Epoch: %d Learning Rate: %.3f\" % (i + 1, sess.run(train_model.lr)))\n",
    "\n",
    "        # run training pass\n",
    "        train_cost, train_perp = run_epoch(sess, train_model, train_data, verbose=True)\n",
    "        print(\"Epoch: %i Training Perplexity: %.3f (Cost: %.3f)\" % (i + 1, train_perp, train_cost))\n",
    "        train_costs.append(train_cost)\n",
    "        train_perps.append(train_perp)\n",
    "\n",
    "        # run validation pass\n",
    "        valid_cost, valid_perplexity = run_epoch(sess, valid_model, valid_data)\n",
    "        print(\"Epoch: %i Validation Perplexity: %.3f (Cost: %.3f)\" % (i + 1, valid_perp, valid_cost))\n",
    "        valid_costs.append(valid_cost)\n",
    "        valid_perps.append(valid_perp)\n",
    "\n",
    "    # run test pass\n",
    "    test_cost, test_perp = run_epoch(sess, test_model, test_data)\n",
    "    print(\"Test Perplexity: %.3f (Cost: %.3f)\" % (test_perp, test_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
